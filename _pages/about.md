---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a 3<sup>rd</sup>-year Ph.D. student at School of Computer Science and Engineering, Nanyang Technological University, supervised by Prof. [Eng Siong Chng](https://scholar.google.com/citations?hl=en&user=FJodrCcAAAAJ). 
Prior to this, I received my B.Eng. degree from University of Science and Technology of China in 2020.

My research topics are:

- **Large Language Model (LLM):** Generative Seq2seq Learning, Multimodal Prompting;

- **Speech Processing:** Speech Recognition&thinsp;/&thinsp;Translation&thinsp;/&thinsp;Synthesis&thinsp;/&thinsp;Enhancement, Efficient Adaptation of Foundation Models;

- **Multimodal:** Audio-Visual Representation Learning;

# üìñ Education
**Nanyang Technological University (NTU)**,&nbsp; *08/2021 - 08/2025*

* Ph.D. in Computer Science. Supervisor: Eng Siong Chng.

**University of Science and Technology of China (USTC)**,&nbsp; *09/2016 - 06/2020*

* B.Eng. in Automation. GPA: 3.76/4.3 (Rank: Top 5%)&nbsp; [[Transcript]](https://yuchen005.github.io/files/transcript_en_zh.pdf)


# üìù Publications 

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span> Self-Taught Recognizer: Toward Adaptive Speech Foundation Models with Unlabeled Data, **<u>Yuchen Hu</u>**, Chen Chen, Chao-Han Huck Yang, Chao Zhang, Pin-Yu Chen, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span> Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models, **<u>Yuchen Hu</u>**, Chen Chen, Chengwei Qin, Qiushi Zhu, Eng Siong Chng, Ruizhe Li.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span> [GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators](https://arxiv.org/abs/2402.06894), **<u>Yuchen Hu</u>**, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/GenTranslate) [[Data]](https://huggingface.co/datasets/PeacefulData/HypoTranslate)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICLR 2024</span> <span style="color:red">(Spotlight, Top 5%)</span> [Large Language Models are Efficient Learners of Noise-Robust Speech Recognition](https://openreview.net/pdf?id=ceATjGPTUD), **<u>Yuchen Hu</u>**, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/RobustGER) [[Data]](https://huggingface.co/datasets/PeacefulData/Robust-HyPoradise)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICLR 2024</span> [It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition](https://openreview.net/pdf?id=QqjFHyQwtF), Chen Chen, Ruizhe Li, **<u>Yuchen Hu</u>**, Sabato Marco Siniscalchi, Pin-Yu Chen, Eng Siong Chng, Chao-Han Huck Yang. 

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">NeurIPS 2023</span> [HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models](https://arxiv.org/abs/2309.15701), Chen Chen\*, **<u>Yuchen Hu</u>\***, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, Eng Siong Chng. [[Code]](https://github.com/Hypotheses-Paradise/Hypo2Trans) [[Data]](https://huggingface.co/datasets/PeacefulData/HP-v0)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">AAAI 2024</span> [Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-modal Speech Representation](https://arxiv.org/abs/2401.03468), Qiushi Zhu, Jie Zhang, Yu Gu, **<u>Yuchen Hu</u>**, Lirong Dai. [[Code]](https://github.com/zqs01/multi-channel-wav2vec2)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2024</span> [Cross-Modality and Within-Modality Regularization for Audio-Visual DeepFake Detection](https://arxiv.org/abs/2401.05746), Heqing Zou, Meng Shen, **<u>Yuchen Hu</u>**, Chen Chen, Eng Siong Chng, Deepu Rajan. [[Code]](https://github.com/Vincent-ZHQ/MRDF)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2024</span> [An Experimental Comparison of Noise-Robust Text-To-Speech Synthesis Systems Based On Self-Supervised Representation](https://yuchen005.github.io/files/zhao2024experimental.pdf), Xiaoying Zhao, Qiushi Zhu, **<u>Yuchen Hu</u>**. [[Demo]](https://zzftts.github.io/)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ACL 2023</span> <span style="color:red">(Oral)</span> [Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition](https://aclanthology.org/2023.acl-long.848.pdf), **<u>Yuchen Hu</u>**, Ruizhe Li, Chen Chen, Chengwei Qin, Qiushi Zhu, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/UniVPM)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ACL 2023</span> <span style="color:red">(Oral)</span> [MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition](https://aclanthology.org/2023.acl-long.649.pdf), **<u>Yuchen Hu</u>**, Chen Chen, Ruizhe Li, Heqing Zou, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/MIR-GAN)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ACL 2023</span> [UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning](https://aclanthology.org/2023.findings-acl.41.pdf), Heqing Zou, Meng Shen, Chen Chen, **<u>Yuchen Hu</u>**, Deepu Rajan, Eng Siong Chng. [[Code]](https://github.com/vincent-zhq/unis-mmc)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">IJCAI 2023</span> [Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition](https://www.ijcai.org/proceedings/2023/0564.pdf), **<u>Yuchen Hu</u>**, Ruizhe Li, Chen Chen, Heqing Zou, Qiushi Zhu, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/GILA)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">AAAI 2023</span> <span style="color:red">(Oral)</span> [Leveraging Modality-specific Representations for Audio-visual Speech Recognition via Reinforcement Learning](https://arxiv.org/abs/2212.05301), Chen Chen, **<u>Yuchen Hu</u>**, Qiang Zhang, Heqing Zou, Beier Zhu, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">IEEE/ACM TASLP 2023</span>  [Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR](https://arxiv.org/abs/2304.04974), **<u>Yuchen Hu</u>**, Chen Chen, Qiushi Zhu, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span> 	
Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System, Chen Chen, Ruizhe Li, **<u>Yuchen Hu</u>**, Yuanyuan Chen, Chengwei Qin, Qiang Zhang.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span>  [Noise-aware Speech Enhancement using Diffusion Probabilistic Model](https://arxiv.org/abs/2307.08029), **<u>Yuchen Hu</u>**, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/NASE)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Preprint</span>  [Rep2wav: Noise Robust Text-to-speech Using Self-supervised Representations](https://arxiv.org/pdf/2308.14553), Qiushi Zhu, Yu Gu, Chao Weng, **<u>Yuchen Hu</u>**, Lirong Dai, Jie Zhang. [[Demo]](https://zqs01.github.io/rep2wav/)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">InterSpeech 2023</span>  [Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition](https://www.isca-archive.org/interspeech_2023/hu23_interspeech.pdf), **<u>Yuchen Hu</u>**, Nana Hou, Chen Chen, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/DPSL-ASR)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">InterSpeech 2023</span>  [A Neural State-space Model Approach to Efficient Speech Separation](https://www.isca-archive.org/interspeech_2023/chen23g_interspeech.pdf), Chen Chen, Chao-Han Huck Yang, Kai Li, **<u>Yuchen Hu</u>**, Pin-Jui Ku, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2023</span>  [Unifying Speech Enhancement and Separation with Gradient Modulation for End-to-End Noise-Robust Speech Separation](https://arxiv.org/abs/2302.11131), **<u>Yuchen Hu</u>**, Chen Chen, Heqing Zou, Xionghu Zhong, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/Unified-Enhance-Separation)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2023</span>  [Gradient Remedy for Multi-Task Learning in End-to-End Noise-Robust Speech Recognition](https://arxiv.org/abs/2302.11362), **<u>Yuchen Hu</u>**, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/Gradient-Remedy)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2023</span>  [Robust Data2vec: Noise-robust Speech Representation Learning for ASR by Combining Regression and Improved Contrastive Learning](https://arxiv.org/abs/2210.15324), Qiushi Zhu, Long Zhou, Jie Zhang, Shujie Liu, **<u>Yuchen Hu</u>**, Lirong Dai. [[Code]](https://github.com/zqs01/data2vecnoisy)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2023</span>  [Metric-oriented Speech Enhancement Using Diffusion Probabilistic Model](https://arxiv.org/abs/2302.11989), Chen Chen, **<u>Yuchen Hu</u>**, Weiwei Weng, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2023</span>  [Unsupervised Noise Adaptation using Data Simulation](https://arxiv.org/abs/2302.11981), Chen Chen, **<u>Yuchen Hu</u>**, Heqing Zou, Linhui Sun, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">InterSpeech 2022</span>  [Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning](https://www.isca-archive.org/interspeech_2022/chen22p_interspeech.pdf), Chen Chen, Nana Hou, **<u>Yuchen Hu</u>**, Heqing Zou, Xiaofeng Qi, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2022</span>  [Interactive Feature Fusion for End-to-End Noise-Robust Speech Recognition](https://arxiv.org/abs/2110.05267), **<u>Yuchen Hu</u>**, Nana Hou, Chen Chen, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/DPSL-ASR)

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2022</span>  [Noise-robust Speech Recognition with 10 Minutes Unparalleled In-domain Data](https://arxiv.org/pdf/2203.15321), Chen Chen, Nana Hou, **<u>Yuchen Hu</u>**, Shashank Shirol, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2022</span>  [Self-Critical Sequence Training for Automatic Speech Recognition](https://arxiv.org/abs/2204.06260), Chen Chen, **<u>Yuchen Hu</u>**, Nana Hou, Xiaofeng Qi, Heqing Zou, Eng Siong Chng.

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">IWSLT 2021</span>  [The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021](https://aclanthology.org/2021.iwslt-1.2.pdf), Dan Liu, Mengge Du, Xiaoxi Li, **<u>Yuchen Hu</u>**, Lirong Dai.


# üíª Internships
Iflytek AI Research &amp; USTC NEL-SLIP, *05/2020 - 07/2021*
* Research intern at ASR group. Supervisor: Lirong Dai.
* Work on end-to-end ASR and speech translation.


# üßë‚Äçüî¨ Services
- **Reviewer:**&nbsp; ACL (23-24), ARR (23-24), EMNLP (23), AAAI (24), ICASSP (22,24), InterSpeech (22-24)
- **Volunteer:**&nbsp; EMNLP (23), ICASSP (22)


# üéñ Honors and Awards
- ACL 2023 Area Chair Award,&nbsp; *07/2023*
- Winner of IWSLT 2021 Evaluation Campaign,&nbsp; *08/2021*
- USTC Excellent Graduate (Top 10%),&nbsp; *06/2020*
- Scholarship of SIMIT, Chinese Academy of Sciences (Top 5%),&nbsp; *10/2018*
- USTC Outstanding Student Scholarship (Top 5%),&nbsp; *10/2017 &amp; 10/2019*



Thanks for the template of <a href="https://github.com/RayeRen/acad-homepage.github.io">Yi Ren</a>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=tWtQE7TmvuyYwZoMvhZ03hz9JwVV2jqLDfST5K4doxk&co=9fc7e3&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>





